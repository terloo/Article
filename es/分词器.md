# 分词器
ES在创建倒排索引时需要对文本进行分词，在搜索时需要对用户输入进行分词。ES默认的分词器对中文分词不友好，需要安装新的分词器

## IK分词器
使用最广的中文ES分词器，支持自定义字典

### 安装
```sh
./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.10.1/elasticsearch-analysis-ik-7.10.1.zip
```

### 分词器类型
1. ik_smart：粗粒度切分
2. ik_max_word：最细粒度切分

### 拓展词库
通过修改`ik分词器插件目录/config/IKAnalyzer.cfg.xml`文件
```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
        <comment>IK Analyzer 扩展配置</comment>
        <!-- 用户可以在这里配置自己的扩展字典 -->
        <!-- ext.dic是文件名，在配置文件同目录，每个词用换行分隔 -->
        <entry key="ext_dict">ext.dic</entry>
        <!-- 用户可以在这里配置自己的扩展停止词字典 -->
        <entry key="ext_stopwords">stopword.dic</entry>
        <!-- 用户可以在这里配置远程扩展字典 -->
        <!-- <entry key="remote_ext_dict">words_location</entry> -->
        <!-- 用户可以在这里配置远程扩展停止词字典 -->
        <!-- <entry key="remote_ext_stopwords">words_location</entry> -->
</properties>
```
